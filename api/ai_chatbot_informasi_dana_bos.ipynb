{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "B9lCgwJuNeHs"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "1. This system that relies on semantic similarity. It finds the text in the document that is most similar to the user's question.\n",
        "2. If the user's question doesn't closely resemble the way the information is expressed in the document, the system may not find the correct answer.\n",
        "3. Basic Functionality covers:\n",
        "    * Extract text from PDF documents.\n",
        "    * Perform semantic search to find relevant chunks of text.\n",
        "    * Clean the output to remove unwanted content.\n",
        "    * Provide an answer to the user's question (even if the answer is not always perfect).\n",
        "\n"
      ],
      "metadata": {
        "id": "B9lCgwJuNeHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further Development\n",
        "1. Clarifying Expectation, example :\n",
        "    * Chatbot: \"Dana BOS digunakan untuk membiayai kegiatan operasional sekolah. Apakah Anda ingin mengetahui contoh kegiatan operasional yang dapat dibiayai oleh Dana BOS?\"\n",
        "2. Provide a list of example questions that the user can ask. This shows them the types of questions the chatbot is good at answering. Example:\n",
        "    * Apa saja syarat pengajuan Dana BOS?\n",
        "    * Bagaimana cara melaporkan penggunaan Dana BOS?\n",
        "    * Sebutkan contoh kegiatan yang dapat dibiayai oleh Dana BOS.\n",
        "3. Keyword Suggestions: As the user types their question, suggest relevant keywords that they can include to make their question more specific.\n",
        "4. Intent Recognition (Advanced): Implement a simple intent recognition system. This would analyze the user's question and try to identify the intent behind it (e.g., \"find allowed uses,\" \"find reporting requirements\"). Based on the intent, the chatbot could automatically rephrase the question to be more targeted. This requires more advanced natural language processing techniques.\n",
        "5. Expand the Training Data (If Possible): If you have the ability to add more data to the system, try to find documents that explicitly list the allowed uses of Dana BOS in a clear and structured way. This will make it easier for the semantic search to find the right information.\n",
        "6. Hybrid Approach (Advanced): Consider combining this semantic search approach with a more traditional keyword-based search. If the semantic search fails to find a good answer, the chatbot could fall back to a keyword search to find any relevant documents and present them to the user."
      ],
      "metadata": {
        "id": "1uA6_DXcA7dk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Library"
      ],
      "metadata": {
        "id": "uMo4iKw2NpWv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omTDcRywLsNw",
        "outputId": "13089383-ccfd-4b96-b802-e5f2eadf3168"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.25.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ],
      "source": [
        "!pip install pymupdf nltk transformers sentence-transformers faiss-cpu\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import fitz\n",
        "import nltk\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "from nltk import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "from google.colab import drive\n",
        "\n",
        "# Download resource NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Gathering"
      ],
      "metadata": {
        "id": "8SFcHCpwORjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Gathering\n",
        "# ===============================\n",
        "# 1. MOUNT GOOGLE DRIVE & CEK FILES\n",
        "# ===============================\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path ke direktori penyimpanan file PDF\n",
        "pdf_dir = \"/content/drive/My Drive/Colab Notebooks/AI Chatbot Berbasis Regulasi\"\n",
        "\n",
        "# Cek apakah direktori ada\n",
        "if not os.path.exists(pdf_dir):\n",
        "    raise FileNotFoundError(f\"Direktori {pdf_dir} tidak ditemukan! Periksa kembali path-nya.\")\n",
        "else:\n",
        "    print(f\"Direktori ditemukan! Daftar file PDF: {os.listdir(pdf_dir)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUAjLu-hOUiX",
        "outputId": "d8d5469c-e977-455a-ec82-b2c7e62b2485"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Direktori ditemukan! Daftar file PDF: ['Permendikbudriset No. 63 Tahun 2023.pdf', 'Untitled folder', 'embeddings.npy', 'chunks.json', 'cleaned_texts.json']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 2. EKSTRAKSI TEKS DARI FILE PDF\n",
        "# ===============================\n",
        "\n",
        "# --- PDF Text Extraction ---\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text(\"text\") + \"\\n\"\n",
        "    return text.strip()\n",
        "\n",
        "pdf_files = [f for f in os.listdir(pdf_dir) if f.endswith(\".pdf\")]\n",
        "pdf_texts = {}\n",
        "\n",
        "for pdf_file in pdf_files:\n",
        "    pdf_path = os.path.join(pdf_dir, pdf_file)\n",
        "    try:\n",
        "        text = extract_text_from_pdf(pdf_path)\n",
        "        pdf_texts[pdf_file] = text\n",
        "        print(f\"Extracted text from: {pdf_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from {pdf_file}: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFfX2D-2Qfsf",
        "outputId": "e05fa41f-bbb7-4498-95dd-0241393045f7"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted text from: Permendikbudriset No. 63 Tahun 2023.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing Data"
      ],
      "metadata": {
        "id": "AQ-aw2LwUTW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 3. PREPROCESSING TEKS\n",
        "# ===============================\n",
        "\n",
        "def clean_text(text):\n",
        "\n",
        "    # This collapses multiple consecutive blank lines into a single blank line,\n",
        "    # reducing unnecessary whitespace.\n",
        "    text = re.sub(r'\\n+', '\\n', text)\n",
        "\n",
        "    # replaces sequences of spaces, tabs, or newlines with a single space,\n",
        "    # ensuring consistent spacing\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # This line finds instances like \"Pasal 17.\" and replaces them with\n",
        "    # \"Pasal 17 \". It removes the dot after the number and ensures\n",
        "    # there is space. This prevents the sentence tokenizer from incorrectly\n",
        "    # splitting \"Pasal 17.\" into two sentences. It's important to keep\n",
        "    # \"Pasal 17\" together as a single unit.\n",
        "    text = re.sub(r'Pasal (\\d+)\\.\\s', r'Pasal \\1 ', text)\n",
        "\n",
        "    # Remove dot, KEEP contents of parentheses\n",
        "    text = re.sub(r'Ayat \\((\\d+[a-z]?)\\)\\.\\s', r'Ayat (\\1) ', text)\n",
        "\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text, flags=re.IGNORECASE)  # Remove URLs\n",
        "    text = re.sub(r'jdih\\.kemdikbud\\.go\\.id', '', text, flags=re.IGNORECASE)  # Remove specific website\n",
        "\n",
        "    # Replace page number pattern '- 4 -' with '(page 4)'\n",
        "    text = re.sub(r'\\s-\\s(\\d+)\\s-\\s', r' (page \\1) ', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "cleaned_texts = {pdf: clean_text(text) for pdf, text in pdf_texts.items()}"
      ],
      "metadata": {
        "id": "KHaUz08k3Fbc"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 4. CHUNKING TEKS\n",
        "# Splits text into smaller chunks.\n",
        "# ===============================\n",
        "\n",
        "def chunk_text(text, chunk_size=500):\n",
        "    sentences = sent_tokenize(text)\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if len(current_chunk) + len(sentence) + 1 <= chunk_size:\n",
        "            current_chunk += sentence + \" \"\n",
        "        else:\n",
        "            if len(current_chunk) > 100:  # Pastikan chunk tidak terlalu kecil\n",
        "                chunks.append(current_chunk.strip())\n",
        "            current_chunk = sentence + \" \"\n",
        "\n",
        "    if len(current_chunk) > 100:  # Pastikan chunk tidak terlalu kecil\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def clean_chunk(chunk):\n",
        "    # Hapus angka atau simbol yang berada di awal baris\n",
        "    chunk = re.sub(r'^\\s*[\\d\\-\\•]+', '', chunk)\n",
        "    # Hapus angka yang berdiri sendiri tanpa konteks\n",
        "    chunk = re.sub(r'\\s*\\d+\\s*$', '', chunk)\n",
        "    return chunk.strip()\n",
        "\n",
        "def filter_irrelevant_text(chunk):\n",
        "    irrelevant_patterns = [\n",
        "        r'\\(\\d+\\)\\s*Dihapus',  # \"(3) Dihapus\", \"(4) Dihapus\"\n",
        "        r'-\\d+-',  # \"-9-\", \"-11-\" (kemungkinan nomor halaman)\n",
        "        r'Pasal\\s*\\d+',  # \"Pasal 52a\" (jika tidak ada konteks)\n",
        "        r'^\\s*\\.\\s*$',  # Tanda titik yang berdiri sendiri\n",
        "    ]\n",
        "\n",
        "    for pattern in irrelevant_patterns:\n",
        "        chunk = re.sub(pattern, '', chunk)\n",
        "\n",
        "    # Hapus angka yang berdiri sendiri, kecuali yang ada dalam kurung ( ) atau { }\n",
        "    chunk = re.sub(r'\\b\\d+\\b(?![\\)}])', ' ', chunk)  # Hanya hapus angka yang tidak diikuti kurung\n",
        "\n",
        "    # Hilangkan spasi berlebihan\n",
        "    chunk = re.sub(r'\\s+', ' ', chunk).strip()\n",
        "\n",
        "    return chunk\n",
        "\n",
        "all_chunks = []\n",
        "for pdf, text in cleaned_texts.items():\n",
        "    chunks = chunk_text(text)  # 1️⃣ Chunking dulu\n",
        "    cleaned_chunks = [clean_chunk(chunk) for chunk in chunks]  # 2️⃣ Clean angka awal\n",
        "    final_chunks = [filter_irrelevant_text(chunk) for chunk in cleaned_chunks]  # 3️⃣ Hapus bagian tidak relevan\n",
        "    all_chunks.extend(final_chunks)\n",
        "\n",
        "print(f\"Total chunks: {len(all_chunks)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ka9AdmeJHERa",
        "outputId": "759474cd-2134-4f54-95c4-7f32c1207730"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total chunks: 66\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SAVING DATA"
      ],
      "metadata": {
        "id": "tNdrpdq1VEse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 5. SAVING DATA\n",
        "# ===============================\n",
        "\n",
        "# Define file paths for saving data\n",
        "chunks_file = os.path.join(pdf_dir, \"chunks.json\")  # Path to save chunks\n",
        "cleaned_texts_file = os.path.join(pdf_dir, \"cleaned_texts.json\") # Path to save cleaned texts\n",
        "\n",
        "# --------------------------------------\n",
        "# 1. Saving the Chunks of Text\n",
        "# --------------------------------------\n",
        "try:\n",
        "    with open(chunks_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(all_chunks, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"Chunks saved to: {chunks_file}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving chunks: {e}\")\n",
        "\n",
        "# --------------------------------------\n",
        "# 2. Saving the Cleaned PDF Texts\n",
        "# --------------------------------------\n",
        "try:\n",
        "    with open(cleaned_texts_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(cleaned_texts, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"Cleaned texts saved to: {cleaned_texts_file}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving cleaned texts: {e}\")"
      ],
      "metadata": {
        "id": "xCHxhNuqVGav",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87aa7087-f3c8-4726-bec1-65c8942bfd3d"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunks saved to: /content/drive/My Drive/Colab Notebooks/AI Chatbot Berbasis Regulasi/chunks.json\n",
            "Cleaned texts saved to: /content/drive/My Drive/Colab Notebooks/AI Chatbot Berbasis Regulasi/cleaned_texts.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOAD MODEL"
      ],
      "metadata": {
        "id": "_TRngQpOU0vv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 6. MODEL\n",
        "# ===============================\n",
        "\n",
        "# Load Sentence Transformer model\n",
        "cross_encoder_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# ===============================\n",
        "# 7. FAISS INDEXING\n",
        "# ===============================\n",
        "\n",
        "d = 384  # Dimensi embedding dari model MiniLM\n",
        "index = faiss.IndexFlatL2(d)\n",
        "chunk_embeddings = embedder.encode(all_chunks, convert_to_numpy=True)\n",
        "index.add(chunk_embeddings)"
      ],
      "metadata": {
        "id": "PL7UayJwU2Za"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Question Answering ---\n",
        "\n",
        "stopwords_id = list(stopwords.words(\"indonesian\"))\n",
        "\n",
        "# Normalisasi stopwords agar cocok dengan tokenizer Scikit-learn\n",
        "vectorizer_temp = TfidfVectorizer()\n",
        "tokenizer = vectorizer_temp.build_tokenizer()\n",
        "stopwords_id = [\" \".join(tokenizer(word)) for word in stopwords_id]  # Pastikan konsistensi\n",
        "\n",
        "def extract_keywords(question, top_n=5):\n",
        "    \"\"\"Mengekstrak kata kunci dari pertanyaan menggunakan TF-IDF (Bahasa Indonesia)\"\"\"\n",
        "    vectorizer = TfidfVectorizer(stop_words=stopwords_id)  # Gunakan stopwords ID yang sudah dinormalisasi\n",
        "    tfidf_matrix = vectorizer.fit_transform([question])\n",
        "\n",
        "    feature_array = np.array(vectorizer.get_feature_names_out())\n",
        "    tfidf_sorting = np.argsort(tfidf_matrix.toarray()).flatten()[::-1]\n",
        "\n",
        "    top_keywords = feature_array[tfidf_sorting][:top_n]\n",
        "    return set(top_keywords)\n",
        "\n",
        "def filter_chunks_by_keywords(question, chunks):\n",
        "    \"\"\"Memilih hanya chunk yang mengandung kata kunci dari pertanyaan\"\"\"\n",
        "    keywords = extract_keywords(question)\n",
        "    filtered_chunks = [chunk for chunk in chunks if any(keyword.lower() in chunk.lower() for keyword in keywords)]\n",
        "    return filtered_chunks if filtered_chunks else chunks  # Jika kosong, tetap gunakan semua chunk\n",
        "\n",
        "def answer_question(question, chunks, top_n=3):\n",
        "    # Filter chunk berdasarkan kata kunci pertanyaan\n",
        "    filtered_chunks = filter_chunks_by_keywords(question, chunks)\n",
        "\n",
        "    # Lakukan embedding hanya pada chunk yang relevan\n",
        "    filtered_embeddings = embedder.encode(filtered_chunks, convert_to_numpy=True)\n",
        "\n",
        "    # Buat indeks FAISS baru untuk chunk yang sudah difilter\n",
        "    index_filtered = faiss.IndexFlatL2(d)\n",
        "    index_filtered.add(filtered_embeddings)\n",
        "\n",
        "    # Cari similarity dengan FAISS\n",
        "    question_embedding = embedder.encode([question], convert_to_numpy=True)\n",
        "    D, I = index_filtered.search(question_embedding, min(top_n * 2, len(filtered_chunks)))\n",
        "\n",
        "    # Ambil kandidat chunk berdasarkan FAISS\n",
        "    candidates = [filtered_chunks[i] for i in I[0]]\n",
        "\n",
        "    # Gunakan Cross-Encoder untuk memilih chunk terbaik\n",
        "    pairs = [(question, chunk) for chunk in candidates]\n",
        "    scores = cross_encoder_model.predict(pairs)\n",
        "    top_indices = np.argsort(scores)[::-1][:top_n]\n",
        "\n",
        "    context = \"\\n\".join([candidates[i] for i in top_indices])\n",
        "    return context\n",
        "\n",
        "def post_process_answer(answer):\n",
        "    sentences = sent_tokenize(answer)\n",
        "\n",
        "    # Hapus duplikasi dan urutkan kalimat agar lebih jelas\n",
        "    unique_sentences = list(dict.fromkeys(sentences))\n",
        "\n",
        "    # Format sebagai bullet list dengan memastikan keterbacaan\n",
        "    bulleted_list = \"\\n\".join([f\"* {sentence.strip()}\" for sentence in unique_sentences if len(sentence.strip()) > 10])\n",
        "\n",
        "    return bulleted_list"
      ],
      "metadata": {
        "id": "WHrZjdeiMLUC"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TESTING\n",
        "Chatbot menampilkan mengambil dan menampilkan 3 chunk teks yang paling mirip dengan pertanyaan pengguna sebagai jawaban."
      ],
      "metadata": {
        "id": "jZ1hT4h7fCl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 7. TESTING CHATBOT\n",
        "# ===============================\n",
        "\n",
        "# --- Example Usage ---\n",
        "question = \"Apakah Dana BOSP dapat digunakan untuk pengembangan sumber daya manusia?\"  # More focused question\n",
        "raw_answer = answer_question(question, all_chunks, top_n=3)\n",
        "processed_answer = post_process_answer(raw_answer)\n",
        "\n",
        "print(f\"Pertanyaan: {question}\")\n",
        "print(f\"Jawaban:\\n{processed_answer}\")\n",
        "\n",
        "# --- Example Usage ---\n",
        "question = \"Untuk apa saja Dana BOS Kinerja dapat digunakan?\"  # More focused question\n",
        "raw_answer = answer_question(question, all_chunks, top_n=3)\n",
        "processed_answer = post_process_answer(raw_answer)\n",
        "\n",
        "print(f\"Pertanyaan: {question}\")\n",
        "print(f\"Jawaban:\\n{processed_answer}\")\n",
        "\n",
        "# --- Example Usage ---\n",
        "question = \"Kapan laporan realisasi penggunaan Dana BOSP harus disampaikan?\"  # More focused question\n",
        "raw_answer = answer_question(question, all_chunks, top_n=3)\n",
        "processed_answer = post_process_answer(raw_answer)\n",
        "\n",
        "print(f\"Pertanyaan: {question}\")\n",
        "print(f\"Jawaban:\\n{processed_answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAASzWSmey1I",
        "outputId": "6f98003c-dd15-4913-d42b-bcca0584c662"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pertanyaan: Apakah Dana BOSP dapat digunakan untuk pengembangan sumber daya manusia?\n",
            "Jawaban:\n",
            "* (2) Komponen penggunaan Dana BOS Kinerja bagi sekolah yang melaksanakan Program Sekolah Penggerak sebagaimana dimaksud pada ayat (1) huruf a meliputi: a. pengembangan sumber daya manusia; b. pembelajaran kurikulum merdeka; c. digitalisasi sekolah; dan d. perencanaan berbasis data.\n",
            "* (2) Penyampaian laporan realisasi penggunaan Dana BOSP sebagaimana dimaksud pada ayat (1) dilaksanakan paling lambat: a. tanggal Juli tahun anggaran berkenaan untuk laporan realisasi pengunaan Dana BOP PAUD Reguler, Dana BOS Reguler, atau Dana BOP Kesetaraan Reguler tahap I yang ada di Satuan Pendidikan; dan b. tanggal Januari tahun anggaran berikutnya untuk laporan realisasi keseluruhan penggunaan Dana BOSP yang diterima dalam satu tahun anggaran.\n",
            "* Ketentuan diubah, sehingga berbunyi sebagai berikut: Komponen penggunaan Dana BOP PAUD Kinerja sebagaimana dimaksud dalam ayat (2) huruf b meliputi: a. pengembangan sumber daya manusia; b. pembelajaran kurikulum merdeka; c. digitalisasi sekolah; dan/atau d. perencanaan berbasis data.\n",
            "Pertanyaan: Untuk apa saja Dana BOS Kinerja dapat digunakan?\n",
            "Jawaban:\n",
            "* Dana Bantuan Operasional Sekolah Kinerja yang selanjutnya disebut Dana BOS Kinerja adalah Dana BOS yang digunakan untuk peningkatan mutu pendidikan Satuan Pendidikan yang menyelenggarakan pendidikan dasar dan pendidikan menengah yang dinilai berkinerja baik.\n",
            "* Dana Bantuan Operasional Penyelenggaraan Pendidikan Anak Usia Dini Kinerja yang selanjutnya disebut Dana BOP PAUD Kinerja adalah Dana BOP PAUD yang digunakan untuk peningkatan mutu pendidikan Satuan Pendidikan yang menyelenggarakan pendidikan anak usia dini yang dinilai berkinerja baik.\n",
            "* Dana Bantuan Operasional Sekolah Reguler yang selanjutnya disebut Dana BOS Reguler adalah Dana BOS yang digunakan untuk membiayai kegiatan operasional rutin Satuan Pendidikan dalam menyelenggarakan pendidikan dasar dan menengah.\n",
            "Pertanyaan: Kapan laporan realisasi penggunaan Dana BOSP harus disampaikan?\n",
            "Jawaban:\n",
            "* (2) Penyampaian laporan realisasi penggunaan Dana BOSP sebagaimana dimaksud pada ayat (1) dilaksanakan paling lambat: a. tanggal Juli tahun anggaran berkenaan untuk laporan realisasi pengunaan Dana BOP PAUD Reguler, Dana BOS Reguler, atau Dana BOP Kesetaraan Reguler tahap I yang ada di Satuan Pendidikan; dan b. tanggal Januari tahun anggaran berikutnya untuk laporan realisasi keseluruhan penggunaan Dana BOSP yang diterima dalam satu tahun anggaran.\n",
            "* (2) Laporan realisasi keseluruhan sebagaimana dimaksud dalam dan laporan realisasi minimal % (lima puluh persen) penggunaan Dana BOP PAUD Reguler, Dana BOS Reguler, atau Dana BOP Kesetaraan Reguler yang diterima pada tahap I menjadi dasar penyaluran tahap II tahun anggaran berkenaan.\n",
            "* Ketentuan ayat (1) diubah, sehingga berbunyi sebagai berikut: (1) Menteri, gubernur, dan bupati/wali kota melakukan pemantauan dan evaluasi sesuai dengan kewenangannya.\n",
            "* Rincian Komponen Penggunaan Dana BOS Kinerja Sekolah yang Memiliki Kemajuan Terbaik a. Pembelajaran kurikulum merdeka merupakan komponen yang digunakan untuk pembiayaan dalam kegiatan pembelajaran bagi Peserta Didik yang berorientasi pada penguatan kompetensi dan pengembangan karakter, seperti: 1) fasilitasi penguatan kompetensi dan pengembangan karakter; 2) fasilitasi evaluasi pembelajaran berbasis rapor pendidikan; dan/atau 3) kegiatan lainnya yang relevan yang mendukung pembelajaran kurikulum merdeka.\n"
          ]
        }
      ]
    }
  ]
}